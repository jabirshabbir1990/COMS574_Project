{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Connect to Google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2EB8WaUqhOL",
        "outputId": "465d826e-96f8-45dd-ca86-6ffd90de8ba1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGV3KHnnWw7I",
        "outputId": "d6e40c07-d891-4a9e-a4a3-63bcdac6a497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-04 21:29:45--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-04 21:29:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-04 21:29:46--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 39s  \n",
            "\n",
            "2025-05-04 21:32:26 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_size = 200;"
      ],
      "metadata": {
        "id": "kf9JfOY3eooF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuOQEJMaSAjE",
        "outputId": "7830b596-bf69-42bb-9056-82fd338f9946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = \"/content/drive/MyDrive/574_Project/glove.6B.200d.txt\"  # Change to \"50d\", \"200d\", or \"300d\" if needed\n",
        "embeddings_index = load_glove_embeddings(glove_path)\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6agbxgpNWkZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f947f70-1016-4b09-b2d6-306edadad589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-9.8809e-01  4.3912e-01 -1.5968e-01 -6.4761e-01  4.5850e-01 -3.8629e-01\n",
            "  3.1324e-01 -1.4723e-01  6.0489e-01  3.4617e-01 -1.7198e-01 -4.7894e-01\n",
            "  5.5329e-01  5.7174e-01  3.3249e-01  3.3135e-01  3.2713e-01 -1.2775e-01\n",
            " -7.5151e-02  1.4600e-01  1.1909e-01  1.2987e+00  1.4428e-01 -8.8560e-02\n",
            " -5.6583e-02 -2.0832e-01  3.7297e-02 -2.7068e-01  3.5929e-01  3.5666e-01\n",
            " -1.3602e-01 -2.6616e-01  5.1051e-02 -3.8901e-01 -7.1688e-01  4.2142e-01\n",
            "  3.4138e-01  2.0226e-01  7.2525e-01 -8.2843e-02 -5.0313e-04  9.6399e-02\n",
            " -9.9015e-01 -2.5072e-01  1.3442e-01 -1.4179e-01  4.5943e-01  1.4141e-01\n",
            "  2.2300e-01  2.5430e-01 -1.4116e-01  1.6485e-01  3.6200e-01  2.7605e-01\n",
            "  4.2591e-01 -7.7762e-01 -3.0578e-01 -3.2377e-01 -1.4614e-01 -3.0976e-01\n",
            "  2.0135e-02  8.5477e-01 -4.5221e-01  2.2912e-01 -1.6643e-02 -8.9288e-02\n",
            "  5.5455e-01 -1.7363e-01  2.6326e-01  2.5399e-01 -6.9577e-01  1.2545e+00\n",
            "  3.8360e-01  6.1884e-01 -1.9624e-01 -6.2276e-01  2.8007e-01 -3.0718e-01\n",
            " -4.5880e-01 -6.4558e-02  1.2010e-01 -3.7269e-01  2.3845e-02  9.3502e-01\n",
            "  6.9278e-01  3.9504e-01 -1.6516e-01  2.6946e-01 -2.2277e-01 -4.2757e-01\n",
            "  2.2187e-01 -1.9731e-01 -4.3778e-01  4.5320e-01 -1.2172e+00  1.9800e-01\n",
            " -1.1947e-01  9.0041e-02  4.2085e-01 -2.0580e-01 -4.3014e-01 -2.3424e-01\n",
            " -2.7362e-01 -6.1372e-01  5.4815e-02 -3.2185e-01  1.4167e-01  1.5766e+00\n",
            " -1.3145e-01  1.4814e-01  5.5194e-01 -2.1592e-01 -4.7255e-01 -2.8905e-01\n",
            "  3.8258e-01 -2.8776e-02 -3.3742e-01 -6.6289e-01  5.9525e-01 -1.1732e-01\n",
            " -6.3757e-02  2.0932e-01  2.7506e-01  9.1853e-03 -2.6480e-02 -9.5169e-01\n",
            " -7.7099e-01  4.8709e-02  5.3862e-01 -1.8867e-01 -2.1653e-01 -1.1950e+00\n",
            " -3.2206e-01  2.0966e-01  2.7707e-01  3.6516e-01 -1.3364e-02  4.9224e-01\n",
            "  1.3911e-01 -6.7729e-01  4.5646e-01  9.0121e-01  1.1623e+00 -2.2724e-02\n",
            "  6.7986e-01  1.8226e-01 -2.3835e-01  2.5362e-01  1.0231e-02  1.1684e-01\n",
            "  3.7366e-01 -6.1280e-01 -1.5657e-01  4.9045e-02  3.4805e-01  1.0667e-01\n",
            "  5.0405e-01 -1.1111e-02  4.3512e-01 -3.5101e-01  6.4006e-02  3.1593e-01\n",
            " -6.2489e-01  2.5810e-01  4.5131e-01 -3.4906e-01  4.8476e-01 -5.0652e-02\n",
            "  6.6812e-01 -2.0489e-01  2.4400e-02  5.1620e-01  7.0946e-02 -7.3994e-03\n",
            " -1.6747e-01  6.8076e-01  3.4721e-01 -2.4126e-02  2.3668e-01 -1.8466e-02\n",
            " -7.9105e-01  1.8090e-01  4.6211e-02 -2.9249e-02 -7.7179e-01 -4.4827e-01\n",
            "  3.0047e-01 -3.5123e-01 -1.9011e-02 -1.1894e-01 -3.5369e-01 -1.2861e-01\n",
            "  1.6508e-01  2.0927e-01  5.5980e-01  3.6686e-01 -1.1306e+00 -1.6466e-01\n",
            "  4.5644e-01 -2.7837e-01]\n"
          ]
        }
      ],
      "source": [
        "word = \"ronald\"\n",
        "if word in embeddings_index:\n",
        "    print(embeddings_index[word])\n",
        "else:\n",
        "    print(\"Word not found in GloVe embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "with open(\"/content/drive/MyDrive/COMS574-Introduction_To_Machine_Learning/News_Category_Dataset_v3.json\", \"r\",encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "#parse json in the files to extract text\n",
        "pattern = '\\{[^{}]*\\}'\n",
        "\n",
        "matches = re.findall(pattern, text)"
      ],
      "metadata": {
        "id": "z_U1gistrpz_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#size of dataset\n",
        "len(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjrMNJfUrs53",
        "outputId": "92718062-0caa-41fb-95f6-18863f2a4fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209527"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter\n",
        "!pip install inflect\n",
        "import inflect\n",
        "from multiprocessing import Pool, cpu_count\n",
        "#Also import all nltk libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import words\n",
        "#download and import dictionary of words\n",
        "nltk.download('words')\n",
        "word_list = words.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdW7_HwKrvRM",
        "outputId": "47b94a16-c940-42c0-8e25-eebe5def4770"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swifter in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.21)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (7.5.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect) (4.4.2)\n",
            "Requirement already satisfied: typing_extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from typeguard>=4.0.1->inflect) (4.13.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create pandas dataframe out of json file\n",
        "import pandas as pd\n",
        "json_objs=[]\n",
        "for item in matches:\n",
        "  try:\n",
        "     json_obj = json.loads(item)\n",
        "     json_objs.append(json_obj)\n",
        "  except json.JSONDecodeError:\n",
        "     print(f\"Could not decode: {item}\")\n",
        "\n",
        "df = pd.DataFrame(json_objs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoMfIIMxrybP",
        "outputId": "35e555a8-5b18-4e83-9e8d-3b0dc5042539"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not decode: {Today's Buddha Doodle}\n",
            "Could not decode: {Today's Buddha Doodle}\n",
            "Could not decode: {This was originally posted to The Screaming\", \"date\": \"2013-08-08\"}\n",
            "Could not decode: {Newlywed}\n",
            "Could not decode: {sic}\n",
            "Could not decode: {EM}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of rows in dataframe\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDxVaGDIr07J",
        "outputId": "45f51317-3079-4383-d1ca-e83c524ffca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "209521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#category counts of ecah category\n",
        "value_counts_column = df['category'].value_counts()\n",
        "print(value_counts_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALx0fdZfr3Df",
        "outputId": "bcfb24c3-f5fc-46aa-f1c2-db49f560c123"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category\n",
            "POLITICS          35602\n",
            "WELLNESS          17945\n",
            "ENTERTAINMENT     17362\n",
            "TRAVEL             9899\n",
            "STYLE & BEAUTY     9813\n",
            "PARENTING          8791\n",
            "HEALTHY LIVING     6694\n",
            "QUEER VOICES       6347\n",
            "FOOD & DRINK       6340\n",
            "BUSINESS           5992\n",
            "COMEDY             5400\n",
            "SPORTS             5077\n",
            "BLACK VOICES       4583\n",
            "HOME & LIVING      4320\n",
            "PARENTS            3955\n",
            "THE WORLDPOST      3664\n",
            "WEDDINGS           3652\n",
            "WOMEN              3572\n",
            "CRIME              3562\n",
            "IMPACT             3484\n",
            "DIVORCE            3426\n",
            "WORLD NEWS         3299\n",
            "MEDIA              2944\n",
            "WEIRD NEWS         2777\n",
            "GREEN              2622\n",
            "WORLDPOST          2579\n",
            "RELIGION           2577\n",
            "STYLE              2254\n",
            "SCIENCE            2206\n",
            "TECH               2104\n",
            "TASTE              2096\n",
            "MONEY              1756\n",
            "ARTS               1509\n",
            "ENVIRONMENT        1444\n",
            "FIFTY              1401\n",
            "GOOD NEWS          1396\n",
            "U.S. NEWS          1377\n",
            "ARTS & CULTURE     1339\n",
            "COLLEGE            1144\n",
            "LATINO VOICES      1130\n",
            "CULTURE & ARTS     1073\n",
            "EDUCATION          1014\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Just taking a subset of 15 categories which are relevant and have good amount of data\n",
        "#Max 5000 rows are selected for eah category.\n",
        "#This is to avoid bias towards particular category.\n",
        "df_subset = pd.DataFrame()\n",
        "categories_shortlisted =['POLITICS','WELLNESS','TRAVEL','STYLE & BEAUTY','SCIENCE','WOMEN','CRIME','BUSINESS','RELIGION'\\\n",
        ",'TECH','U.S. NEWS','WORLD NEWS','ENTERTAINMENT','HEALTHY LIVING','FOOD & DRINK']\n",
        "\n",
        "for category in categories_shortlisted:\n",
        "  temp_df = df[df['category'] == category]\n",
        "  if len(temp_df) >=5000:\n",
        "    temp_df = temp_df.sample(frac=5000/len(temp_df));\n",
        "  df_subset = pd.concat([df_subset,temp_df],ignore_index=True)\n",
        "\n",
        "print(len(df_subset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B_FBkmNr5aD",
        "outputId": "024bc19c-f31e-4bda-9ff8-1e81ee2f320c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "setwords = set(word_list)\n",
        "p = inflect.engine()\n",
        "#If its a dictionary word then just return the lowercase so its not identified as named entity.\n",
        "#This is required for named entity recognition as the dictionary words should not be identified as named entities\n",
        "def checkifItsDictionaryWord(token):\n",
        "   word = copy.deepcopy(token)\n",
        "   #Inorder to check if its a dictionary word, do some preprocessing\n",
        "   #removal of 's or s' for any special characters.\n",
        "   #also remove the inflections such as plural form.\n",
        "   if \"'s\" in word:\n",
        "     word.replace(\"'s\",\"\")\n",
        "   elif \"s'\" in word:\n",
        "     word.replace(\"s'\",\"\")\n",
        "\n",
        "   res = p.singular_noun(word)\n",
        "   #print(res)\n",
        "   if res!=False:\n",
        "      word = res\n",
        "\n",
        "   word = re.sub(r'[^a-zA-Z0-9]', '', word)\n",
        "   if word.lower() in setwords:\n",
        "     del word;\n",
        "     return token.lower()\n",
        "   else:\n",
        "     del word;\n",
        "     return token\n",
        "\n"
      ],
      "metadata": {
        "id": "52U6zKE4r8l1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "t-rdzKc9sJwg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert words to lower case if its a dictionary word so that its not falsely identified as named entity.\n",
        "def convertWordsToLowerCase(sentence):\n",
        "   tokens = [checkifItsDictionaryWord(token)  for token in sentence.split()]\n",
        "\n",
        "   sentence = \" \".join([token.lower() if token.lower() in setwords else token for token in tokens])\n",
        "   return sentence\n",
        "\n",
        "sentence = \"Cars are best vehicles for Kate's travel\"\n",
        "print(convertWordsToLowerCase(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvup5cXBsNIJ",
        "outputId": "be5da8cd-2560-4b9c-91bd-5251586c831e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cars are best vehicles for Kate's travel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since all the words in headline are starting with CAPS we want to change it to lower case if its a dictionary word\n",
        "#So that its not falsely identified as named entity\n",
        "df_subset['headline'] = df_subset['headline'].apply(convertWordsToLowerCase)"
      ],
      "metadata": {
        "id": "a_ogWg3ZsT6s"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally concatenate the headline and short description.\n",
        "df_subset['news_text'] = df_subset['headline']+ \" \"+df_subset['short_description']"
      ],
      "metadata": {
        "id": "kZYQCQLusXd3"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replaceChars(word):\n",
        "  if \"'s\" in word:\n",
        "     word.replace(\"'s\",\"\")\n",
        "  elif \"s'\" in word:\n",
        "     word.replace(\"s'\",\"\")\n",
        "  return word;\n",
        "\n",
        "def removeApostrophe(sent):\n",
        "   tokens = sent.split()\n",
        "   return \" \".join([replaceChars(token) for token in tokens])\n",
        "\n",
        "df_subset['news_text'] = df_subset['news_text'].apply(removeApostrophe)"
      ],
      "metadata": {
        "id": "X6cvZx15r6Eq"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "#function to remove unicode characters\n",
        "def remove_unicode(text):\n",
        "    #print(text)\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "#function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    #text = \"This is a simple sentence that contains some stop words.\"\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Get the list of stop words in English\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove stop words from the text\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the filtered words back into a sentence\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text\n",
        "\n",
        "#function to perform stemming and lemmatization.\n",
        "def performLemmatizationAndStemming(text):\n",
        "    words = text.split()\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            lemmatized_words.append(lemmatizer.lemmatize(word,pos='v'))\n",
        "        except:\n",
        "            print('exception thrown')\n",
        "            print(word)\n",
        "            sys.exit()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # Assuming verbs for simplicity\n",
        "\n",
        "    # Stemming\n",
        "    #stemmer = PorterStemmer()\n",
        "\n",
        "    #stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "    #stemmed_words = ' '.join(stemmed_words)\n",
        "    lemmatized_words = ' '.join(lemmatized_words)\n",
        "    return lemmatized_words\n"
      ],
      "metadata": {
        "id": "jR9K86TEsdm9"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pYbkDmwiJI3",
        "outputId": "7a22130a-fa3f-43a7-8cc9-f224f2dfcec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#not starting with A-Z or 0-9 delete it\n",
        "df_subset['news_text'] = df_subset['news_text'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
        "#remove unicode\n",
        "df_subset['news_text'] = df_subset['news_text'].apply(remove_unicode)\n",
        "#df_subset['news_text'] = df_subset['news_text'].apply(remove_nondictionary_words)\n",
        "#replace \\n with ''\n",
        "df_subset['news_text'] = df_subset['news_text'].str.replace(r'[\\n]','',regex=True)"
      ],
      "metadata": {
        "id": "r5poWV_3s40O"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stop words\n",
        "df_subset['news_text'] = df_subset['news_text'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "_0tSD_6gs7yq"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text'] = df_subset['news_text'].apply(performLemmatizationAndStemming)"
      ],
      "metadata": {
        "id": "O4wsDbnftENj"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text'] = df_subset['news_text'].str.lower()"
      ],
      "metadata": {
        "id": "PZ2NPfdHx0tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "HB7bXA6Fve4C",
        "outputId": "17a27c7b-c35c-4097-8ebf-7a585585e7cc"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    backlash discrimination GOPs Indiana problem 2...\n",
              "1    Chelsea man begin hunger strike need help get ...\n",
              "2    dozens lawmakers urge Biden pick deb Haaland i...\n",
              "3    heres NYC corrections could change Rikers isla...\n",
              "4    save democracy prestige US Supreme Court issue...\n",
              "Name: news_text, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>backlash discrimination GOPs Indiana problem 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Chelsea man begin hunger strike need help get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dozens lawmakers urge Biden pick deb Haaland i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>heres NYC corrections could change Rikers isla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>save democracy prestige US Supreme Court issue...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import re\n"
      ],
      "metadata": {
        "id": "e2O1GoTdj59-"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset = df_subset.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "0rTHzyE3PqgN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = df_subset['news_text'].tolist()"
      ],
      "metadata": {
        "id": "w0Zl0DeykUai"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_maxlen(sequences, maxlen, padding_value=0):\n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < maxlen:\n",
        "            # Pad at end\n",
        "            pad_len = maxlen - len(seq)\n",
        "            padded_seq = torch.cat([seq, torch.full((pad_len,), int(padding_value))])\n",
        "            padded_seq = padded_seq.to(torch.int)\n",
        "        else:\n",
        "            # Truncate\n",
        "            padded_seq = seq[:maxlen]\n",
        "            padded_seq = padded_seq.to(torch.int)\n",
        "        padded.append(padded_seq)\n",
        "    return torch.stack(padded)\n",
        "\n",
        "sequences = [torch.tensor([1, 2, 3]),\n",
        "             torch.tensor([4, 5]),\n",
        "             torch.tensor([6])]\n",
        "\n",
        "padded = pad_to_maxlen(sequences, maxlen=4)\n",
        "print(padded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smkExNUHihTm",
        "outputId": "d1df47a0-d85b-410b-bb49-bd4d3d0f0f4d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3, 0],\n",
            "        [4, 5, 0, 0],\n",
            "        [6, 0, 0, 0]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter(word for sentence in sentence_list for word in sentence)\n",
        "vocab = {word: i+2 for i, (word, _) in enumerate(vocab.most_common())}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "import sys\n",
        "def encode(sentence):\n",
        "    return [vocab.get(word, vocab['<UNK>']) for word in sentence]\n",
        "\n",
        "maxlen = max([ len([word for word in sentence]) for sentence in sentence_list])\n",
        "#print(maxlen)\n",
        "#sys.exit()\n",
        "\n",
        "encoded_sentences = [torch.tensor(encode(sentence)) for sentence in sentence_list]\n",
        "\n",
        "# Padding\n",
        "#padded_sentences = pad_sequence(encoded_sentences, batch_first=True, padding_value=vocab['<PAD>'],maxlen= 200)\n",
        "padded_sentences = pad_to_maxlen(encoded_sentences,200,vocab['<PAD>'])\n",
        "#print(padded_sentences[0])\n",
        "#print(encoded_sentences[0])\n",
        "print(padded_sentences[0])\n",
        "#print([len([word for word in sent]) for sent in padded_sentences])\n",
        "#sys.exit()\n",
        "\n",
        "# Encode labels\n",
        "labels = df_subset['category'].tolist()\n",
        "one_hot = pd.get_dummies(df_subset['category'],dtype =float)\n",
        "#rint(one_hot)\n",
        "#sys.exit()\n",
        "one_hot_tensor = one_hot.to_numpy()\n",
        "print(one_hot_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bra5suhOj2MK",
        "outputId": "0564b5a2-f9d9-460e-b019-858231ff7eea"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([28,  3, 10,  3, 11,  9, 16,  3,  2, 26,  6, 14, 41,  2, 12,  9, 24,  3,\n",
            "         6,  2, 17,  4,  6, 16,  3,  6,  8,  2, 22,  4, 41,  4,  4,  6,  2, 15,\n",
            "         4, 18,  2, 44, 34, 37, 44,  2,  6,  3, 24,  3,  4, 11,  2, 20,  4,  8,\n",
            "        17,  7,  9, 10,  2,  9, 22,  8,  3,  8,  8,  7,  9, 10,  2, 28, 33, 31,\n",
            "        29, 31, 25,  2, 25,  3,  3,  2, 28,  3, 10,  3, 11,  9, 16,  3,  8,  2,\n",
            "         8,  5, 18, 11,  3,  2,  3, 24,  9, 11, 14,  5,  7,  9, 10,  2, 22,  6,\n",
            "         3,  4,  5, 17,  3,  2,  8, 10,  4, 12, 23,  2,  4,  8,  7, 13,  3,  2,\n",
            "        26,  6, 14, 41,  2, 16,  6,  3,  5,  5, 18,  2, 12,  9, 15, 20,  9,  6,\n",
            "         5,  4, 22, 11,  3,  2, 17,  7, 19, 17,  2, 20,  4,  8, 17,  7,  9, 10,\n",
            "         2,  5,  3, 11, 11,  2, 35,  4, 41,  4,  4,  6,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0], dtype=torch.int32)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_5fgmfvRQ3Y"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIocu03JRwn7",
        "outputId": "db264fa8-a8c5-463c-a9a5-5db8ae038a28"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([29,  3, 10, 10,  3,  8,  8,  3,  3,  2,  4,  8,  8,  9, 12,  7,  4,  5,\n",
            "         7,  9, 10,  2,  4, 15,  3, 10, 13,  2,  6, 14, 11,  3,  2, 33,  7, 47,\n",
            "         4, 22, 43,  3,  4,  6,  7, 10, 19,  2,  8, 12, 17,  9,  9, 11,  2,  4,\n",
            "         5, 17, 11,  3,  5,  3,  8,  2, 25,  5, 14, 13,  3, 10,  5,  8,  2, 21,\n",
            "         3,  4,  6,  2, 17,  3,  4, 13,  8, 12,  4,  6, 24,  3,  8,  2, 11,  9,\n",
            "        10, 19,  3,  6,  2,  6,  3, 51, 14,  7,  6,  3,  2,  8,  3,  3, 23,  2,\n",
            "        21,  4,  7, 24,  3,  6,  2, 16,  4,  6,  5,  7, 12,  7, 16,  4,  5,  3,\n",
            "         2,  8, 16,  9,  6,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4xj3CLcYAdS",
        "outputId": "1e6b36d6-b290-4b56-9d75-c9af2fbf87c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4,  1,  7,  ..., 12,  0,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings_index['hello'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MCJz4MA7rsH",
        "outputId": "50129efa-709e-4aa4-ba35-712dad9a294a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 200\n",
        "embedding_matrix = torch.zeros(len(vocab), embedding_dim)\n",
        "\n",
        "for word, idx in vocab.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[idx] = torch.from_numpy(embeddings_index[word]).float()\n",
        "\n",
        "    else:\n",
        "        embedding_matrix[idx] = torch.randn(embedding_dim)  # Random init for unknowns\n"
      ],
      "metadata": {
        "id": "Y9U7nk3Q68qg"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENMGupKl4zgl",
        "outputId": "8af3a3c8-2276-4661-ad9e-98d56c3d093a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[35,  9, 10,  ...,  0,  0,  0],\n",
            "        [48,  3, 22,  ...,  0,  0,  0],\n",
            "        [ 8,  9,  6,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [ 6,  3, 13,  ...,  0,  0,  0],\n",
            "        [26,  4, 11,  ...,  0,  0,  0],\n",
            "        [21,  4,  8,  ...,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded_sentences, one_hot_tensor, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = SentenceDataset(X_train, y_train)\n",
        "test_dataset = SentenceDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "80oXVn_GnrXM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False);\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim, 25)\n",
        "\n",
        "        self.fc2 = nn.Linear(25, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        fc1_out = self.fc1(hn[-1])\n",
        "        fc1_relu = nn.ReLU()\n",
        "        fc1_out = fc1_relu(fc1_out)\n",
        "        fc2_out = self.fc2(fc1_out)\n",
        "        return fc2_out;\n"
      ],
      "metadata": {
        "id": "gBf7rbyEpfpV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from itertools import chain\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LSTMClassifier(len(vocab), embed_dim=200, hidden_dim=256, output_dim=one_hot_tensor.shape[1]).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(60):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    targets_entire_batch = []\n",
        "    indices_entire_batch =[]\n",
        "    accuracy_epoch =0\n",
        "    total_size =0\n",
        "    for inputs, targets in train_loader:\n",
        "        #print(targets)\n",
        "        #sys.exit()\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        #print(probs)\n",
        "        #print(targets)\n",
        "        #sys.exit()\n",
        "        values, indices = torch.max(probs, dim=1)\n",
        "        #print(indices.shape)\n",
        "        #num_classes = indices.size()[0]\n",
        "        #sys.exit()\n",
        "        #predicted_one_hot = F.one_hot(indices, num_classes=15)\n",
        "        target_normal = torch.argmax(targets,dim=1)\n",
        "        #print(target_normal)\n",
        "        #print(indices)\n",
        "        correct = (indices == target_normal).sum().item()\n",
        "        #accuracy = correct / targets.size(0)\n",
        "        accuracy_epoch = accuracy_epoch+correct\n",
        "        total_size = total_size+targets.size(0)\n",
        "        #print(accuracy)\n",
        "        #print(accuracy)\n",
        "        #sys.exit()\n",
        "        #targets_entire_batch.append(targets)\n",
        "        #indices_entire_batch.append(predicted_one_hot)\n",
        "        # Step 3: Compute accuracy\n",
        "\n",
        "\n",
        "        #sys.exit()\n",
        "        loss = criterion(outputs, targets)\n",
        "        #print(loss)\n",
        "        #sys.exit()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    total_loss /= num_batches\n",
        "    accuracy_epoch = accuracy_epoch/total_size;\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy_epoch:.4f}\")\n",
        "    #sys.exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_O6kRQk_jLB",
        "outputId": "d6c750e3-1d9a-4295-cc18-fdf3d47e88cd"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.6237\n",
            "Epoch 1, Accuracy: 0.1073\n",
            "Epoch 2, Loss: 2.5820\n",
            "Epoch 2, Accuracy: 0.1363\n",
            "Epoch 3, Loss: 2.5487\n",
            "Epoch 3, Accuracy: 0.1563\n",
            "Epoch 4, Loss: 2.5072\n",
            "Epoch 4, Accuracy: 0.1754\n",
            "Epoch 5, Loss: 2.4079\n",
            "Epoch 5, Accuracy: 0.2102\n",
            "Epoch 6, Loss: 2.3532\n",
            "Epoch 6, Accuracy: 0.2260\n",
            "Epoch 7, Loss: 2.2937\n",
            "Epoch 7, Accuracy: 0.2501\n",
            "Epoch 8, Loss: 2.1431\n",
            "Epoch 8, Accuracy: 0.2998\n",
            "Epoch 9, Loss: 2.0236\n",
            "Epoch 9, Accuracy: 0.3405\n",
            "Epoch 10, Loss: 1.9257\n",
            "Epoch 10, Accuracy: 0.3713\n",
            "Epoch 11, Loss: 1.8237\n",
            "Epoch 11, Accuracy: 0.4025\n",
            "Epoch 12, Loss: 1.7265\n",
            "Epoch 12, Accuracy: 0.4393\n",
            "Epoch 13, Loss: 1.6249\n",
            "Epoch 13, Accuracy: 0.4731\n",
            "Epoch 14, Loss: 1.5266\n",
            "Epoch 14, Accuracy: 0.5071\n",
            "Epoch 15, Loss: 1.4344\n",
            "Epoch 15, Accuracy: 0.5388\n",
            "Epoch 16, Loss: 1.3493\n",
            "Epoch 16, Accuracy: 0.5649\n",
            "Epoch 17, Loss: 1.2723\n",
            "Epoch 17, Accuracy: 0.5896\n",
            "Epoch 18, Loss: 1.2015\n",
            "Epoch 18, Accuracy: 0.6122\n",
            "Epoch 19, Loss: 1.1303\n",
            "Epoch 19, Accuracy: 0.6351\n",
            "Epoch 20, Loss: 1.0655\n",
            "Epoch 20, Accuracy: 0.6551\n",
            "Epoch 21, Loss: 1.0107\n",
            "Epoch 21, Accuracy: 0.6705\n",
            "Epoch 22, Loss: 0.9548\n",
            "Epoch 22, Accuracy: 0.6870\n",
            "Epoch 23, Loss: 0.8993\n",
            "Epoch 23, Accuracy: 0.7054\n",
            "Epoch 24, Loss: 0.8497\n",
            "Epoch 24, Accuracy: 0.7171\n",
            "Epoch 25, Loss: 0.8031\n",
            "Epoch 25, Accuracy: 0.7323\n",
            "Epoch 26, Loss: 0.7553\n",
            "Epoch 26, Accuracy: 0.7487\n",
            "Epoch 27, Loss: 0.7156\n",
            "Epoch 27, Accuracy: 0.7623\n",
            "Epoch 28, Loss: 0.6807\n",
            "Epoch 28, Accuracy: 0.7720\n",
            "Epoch 29, Loss: 0.6456\n",
            "Epoch 29, Accuracy: 0.7823\n",
            "Epoch 30, Loss: 0.6086\n",
            "Epoch 30, Accuracy: 0.7942\n",
            "Epoch 31, Loss: 0.5799\n",
            "Epoch 31, Accuracy: 0.8032\n",
            "Epoch 32, Loss: 0.5614\n",
            "Epoch 32, Accuracy: 0.8096\n",
            "Epoch 33, Loss: 0.5311\n",
            "Epoch 33, Accuracy: 0.8186\n",
            "Epoch 34, Loss: 0.5015\n",
            "Epoch 34, Accuracy: 0.8285\n",
            "Epoch 35, Loss: 0.4942\n",
            "Epoch 35, Accuracy: 0.8291\n",
            "Epoch 36, Loss: 0.4647\n",
            "Epoch 36, Accuracy: 0.8400\n",
            "Epoch 37, Loss: 0.4537\n",
            "Epoch 37, Accuracy: 0.8432\n",
            "Epoch 38, Loss: 0.4313\n",
            "Epoch 38, Accuracy: 0.8510\n",
            "Epoch 39, Loss: 0.4239\n",
            "Epoch 39, Accuracy: 0.8552\n",
            "Epoch 40, Loss: 0.4085\n",
            "Epoch 40, Accuracy: 0.8579\n",
            "Epoch 41, Loss: 0.3931\n",
            "Epoch 41, Accuracy: 0.8642\n",
            "Epoch 42, Loss: 0.3849\n",
            "Epoch 42, Accuracy: 0.8672\n",
            "Epoch 43, Loss: 0.3730\n",
            "Epoch 43, Accuracy: 0.8712\n",
            "Epoch 44, Loss: 0.3679\n",
            "Epoch 44, Accuracy: 0.8715\n",
            "Epoch 45, Loss: 0.3449\n",
            "Epoch 45, Accuracy: 0.8794\n",
            "Epoch 46, Loss: 0.3316\n",
            "Epoch 46, Accuracy: 0.8826\n",
            "Epoch 47, Loss: 0.3384\n",
            "Epoch 47, Accuracy: 0.8822\n",
            "Epoch 48, Loss: 0.3277\n",
            "Epoch 48, Accuracy: 0.8861\n",
            "Epoch 49, Loss: 0.3213\n",
            "Epoch 49, Accuracy: 0.8882\n",
            "Epoch 50, Loss: 0.3159\n",
            "Epoch 50, Accuracy: 0.8900\n",
            "Epoch 51, Loss: 0.3002\n",
            "Epoch 51, Accuracy: 0.8947\n",
            "Epoch 52, Loss: 0.3002\n",
            "Epoch 52, Accuracy: 0.8933\n",
            "Epoch 53, Loss: 0.2868\n",
            "Epoch 53, Accuracy: 0.8994\n",
            "Epoch 54, Loss: 0.2854\n",
            "Epoch 54, Accuracy: 0.9011\n",
            "Epoch 55, Loss: 0.2711\n",
            "Epoch 55, Accuracy: 0.9057\n",
            "Epoch 56, Loss: 0.2807\n",
            "Epoch 56, Accuracy: 0.9016\n",
            "Epoch 57, Loss: 0.2751\n",
            "Epoch 57, Accuracy: 0.9051\n",
            "Epoch 58, Loss: 0.2765\n",
            "Epoch 58, Accuracy: 0.9039\n",
            "Epoch 59, Loss: 0.2623\n",
            "Epoch 59, Accuracy: 0.9094\n",
            "Epoch 60, Loss: 0.2573\n",
            "Epoch 60, Accuracy: 0.9104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()  # set to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():  # no gradient needed during inference\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            output = model(batch_X).squeeze()\n",
        "            loss = criterion(output, batch_y)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            values, indices = torch.max(probs, dim=1)\n",
        "            target_normal = torch.argmax(targets,dim=1)\n",
        "            correct = (indices == target_normal).sum().item()\n",
        "            accuracy = accuracy+correct\n",
        "            total_loss += loss.item() * batch_X.size(0)  # accumulate loss\n",
        "            predictions.append(output)\n",
        "            actuals.append(batch_y)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "    predictions = torch.cat(predictions)\n",
        "    actuals = torch.cat(actuals)\n",
        "    print(\"The accuracy is\")\n",
        "    print(accuracy/len(test_loader.dataset))\n",
        "    #return avg_loss, predictions, actuals\n",
        "evaluate(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA2gLiH_vSAP",
        "outputId": "77dbf588-d60d-4289-96f7-89be14aa46f3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is\n",
            "0.7509369676320272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBeSmA-2vp40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "def getEmbeddingsNews(sent):\n",
        "  tokens = sent.split()\n",
        "  #print(tokens)\n",
        "  vector_sum = np.zeros(100)\n",
        "  vector_temp = np.zeros(embeddings_size)\n",
        "  mapped_data = map(lambda x: (x, 1), tokens)\n",
        "  result = reduce(lambda acc, x: (acc[0] + embeddings_index[x[0]], acc[1]+x[1]) if x[0] in embeddings_index else acc, mapped_data, (0,0))\n",
        "  #print(result[1])\n",
        "  if result[1] == 0:\n",
        "    print(tokens)\n",
        "    print(sent)\n",
        "    return vector_temp\n",
        "  return result[0]/result[1]"
      ],
      "metadata": {
        "id": "oJgfhyMvzo9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text_embeddings'] = df_subset['news_text'].apply(getEmbeddingsNews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBFI4UkO55qG",
        "outputId": "1769b532-d277-46cc-cf3f-59ac1de22473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "\n",
            "['ahhhhhh']\n",
            "ahhhhhh\n",
            "[]\n",
            "\n",
            "['intomesee']\n",
            "intomesee\n",
            "[]\n",
            "\n",
            "['ladyinwaiting']\n",
            "ladyinwaiting\n",
            "['metabolenvy']\n",
            "metabolenvy\n",
            "['romantical']\n",
            "romantical\n",
            "[]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "w0WcBXkoid8D",
        "outputId": "e1a6a887-daad-4fa6-e293-d9d7c9401492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [-0.13869067, 0.34021485, 0.40968797, 0.031654...\n",
              "1    [0.12643987, 0.17112476, 0.494685, -0.14378382...\n",
              "2    [-0.07266141, 0.36068875, 0.34253892, -0.03346...\n",
              "3    [-0.16239074, 0.3124193, 0.2653063, -0.2430215...\n",
              "4    [-0.19659296, -0.1122778, 0.39440867, 0.031872...\n",
              "Name: news_text_embeddings, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_text_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-0.13869067, 0.34021485, 0.40968797, 0.031654...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0.12643987, 0.17112476, 0.494685, -0.14378382...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.07266141, 0.36068875, 0.34253892, -0.03346...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.16239074, 0.3124193, 0.2653063, -0.2430215...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.19659296, -0.1122778, 0.39440867, 0.031872...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text_embeddings'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "iseoBOeV8dP1",
        "outputId": "5619149f-2572-4031-8068-08e59dc8922d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [0.14322297, 0.29859707, 0.07154488, -0.049467...\n",
              "1    [-0.31627104, 0.106928304, -0.2191517, -0.2303...\n",
              "2    [-0.10568069, 0.30309823, -0.0054959967, -0.03...\n",
              "3    [0.027149007, 0.05887452, -0.10680344, 0.05987...\n",
              "4    [-0.022456072, 0.14850293, -0.22473784, 0.0530...\n",
              "Name: news_text_embeddings, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_text_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0.14322297, 0.29859707, 0.07154488, -0.049467...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-0.31627104, 0.106928304, -0.2191517, -0.2303...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-0.10568069, 0.30309823, -0.0054959967, -0.03...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.027149007, 0.05887452, -0.10680344, 0.05987...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-0.022456072, 0.14850293, -0.22473784, 0.0530...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset = df_subset.sample(frac =1.0, random_state=42);"
      ],
      "metadata": {
        "id": "ZWFv-ZpCxmSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "categories = set(df_subset['category'].unique())\n",
        "categories_list = list(categories)\n",
        "categories_index = [categories_list.index(category) for category in df_subset['category']]"
      ],
      "metadata": {
        "id": "Qno2GK-GxoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array_df_subset = df_subset['news_text_embeddings'].values\n",
        "list_df_subset = list(array_df_subset)"
      ],
      "metadata": {
        "id": "BxBVB5kfi8lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text_embeddings'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqGw31Djdmt",
        "outputId": "f4f645ba-c3b5-437f-8e27-72bfcb6af4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.38690665e-01,  3.40214849e-01,  4.09687966e-01,  3.16547640e-02,\n",
              "       -1.16282105e-01, -9.95175242e-02, -1.21507749e-01, -2.41518617e-02,\n",
              "       -5.44838086e-02, -1.45383179e-03, -5.91842867e-02, -1.49804562e-01,\n",
              "        2.47853324e-02, -1.37979567e-01, -8.77080392e-03, -2.63854712e-01,\n",
              "        3.58655825e-02,  5.54210171e-02, -2.97050625e-01,  1.37238884e-02,\n",
              "        2.08153620e-01,  8.45340937e-02,  9.29854140e-02,  8.45331177e-02,\n",
              "       -1.53387576e-01, -9.32770818e-02,  3.15355286e-02, -5.28422058e-01,\n",
              "        5.96001074e-02, -9.00440961e-02,  3.70324701e-02,  3.50540012e-01,\n",
              "        4.65538760e-04, -4.44586128e-02,  6.80771694e-02,  2.37027466e-01,\n",
              "        7.10499734e-02,  1.61411166e-01, -1.72917247e-01,  1.00211568e-01,\n",
              "       -5.22332370e-01, -2.56896585e-01,  3.04283034e-02, -2.31903449e-01,\n",
              "       -2.25978315e-01, -9.90635604e-02,  1.34553552e-01,  7.54389688e-02,\n",
              "       -3.98006886e-01, -6.61410570e-01,  1.99954603e-02, -2.32198402e-01,\n",
              "       -6.10017926e-02,  8.33103418e-01,  9.27346870e-02, -1.69211626e+00,\n",
              "        3.66243899e-01, -1.91721648e-01,  1.34058130e+00,  1.89201802e-01,\n",
              "       -7.57825673e-02,  2.96839356e-01, -2.19603524e-01, -2.29132310e-01,\n",
              "        6.01252317e-01,  2.13572681e-01,  1.15419596e-01,  3.90886143e-02,\n",
              "        2.75955468e-01, -3.57320189e-01,  9.10351053e-02, -2.81509519e-01,\n",
              "       -4.68449034e-02, -2.39217766e-02,  8.74016136e-02, -6.13143966e-02,\n",
              "       -1.07633322e-01, -7.25450814e-02, -5.63979030e-01, -1.62663817e-01,\n",
              "        2.89981216e-01,  8.60668272e-02, -1.48059696e-01,  2.19462603e-01,\n",
              "       -1.01272786e+00, -1.59249276e-01,  5.54920919e-03, -2.71856710e-02,\n",
              "       -2.01686814e-01, -2.27479115e-01,  1.25032350e-01, -6.59082755e-02,\n",
              "        3.53443734e-02, -8.41757953e-02, -4.62334037e-01,  1.97097838e-01,\n",
              "       -1.36008486e-01, -1.50566742e-01,  4.46058422e-01,  1.31446943e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_model = PCA(n_components=0.90)\n",
        "list_df_subset_reduced = pca_model.fit_transform(list_df_subset)"
      ],
      "metadata": {
        "id": "wZ2FneRJ5LsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "vkkiY1EJoO9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(list_df_subset_reduced, categories_index, test_size=0.2, random_state=100)\n",
        "\n",
        "# Define the kernels to be used\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "kernel_chosen = kernels[2]\n",
        "clf = SVC(kernel=kernel_chosen,C=1.7)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "y_pred = clf.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy with {kernel_chosen} kernel: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gzx15WeoLey",
        "outputId": "eb8d5f9e-c148-4943-a25b-655430f5c153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with rbf kernel: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(list_df_subset, categories_index, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjUzOxHjpBaY",
        "outputId": "dd4a7308-2713-499d-ef03-664b41e63f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}