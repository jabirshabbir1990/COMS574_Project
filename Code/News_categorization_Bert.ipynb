{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Connect to Google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2EB8WaUqhOL",
        "outputId": "8f5ae724-74a2-41d1-9b01-9f3a4770d555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGV3KHnnWw7I",
        "outputId": "37ad8935-788b-49c0-cc3f-f455ac3001aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 01:25:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-05 01:25:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-05 01:25:27--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip          3%[                    ]  26.99M  16.4MB/s               ^C\n",
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_size = 200;"
      ],
      "metadata": {
        "id": "kf9JfOY3eooF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuOQEJMaSAjE",
        "outputId": "8aafca30-8179-4c21-d248-2bfc8f676812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = \"/content/drive/MyDrive/574_Project/glove.6B.200d.txt\"  # Change to \"50d\", \"200d\", or \"300d\" if needed\n",
        "embeddings_index = load_glove_embeddings(glove_path)\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6agbxgpNWkZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc2c3ca-f192-41c9-e4c1-4c9b8c3c5af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-9.8809e-01  4.3912e-01 -1.5968e-01 -6.4761e-01  4.5850e-01 -3.8629e-01\n",
            "  3.1324e-01 -1.4723e-01  6.0489e-01  3.4617e-01 -1.7198e-01 -4.7894e-01\n",
            "  5.5329e-01  5.7174e-01  3.3249e-01  3.3135e-01  3.2713e-01 -1.2775e-01\n",
            " -7.5151e-02  1.4600e-01  1.1909e-01  1.2987e+00  1.4428e-01 -8.8560e-02\n",
            " -5.6583e-02 -2.0832e-01  3.7297e-02 -2.7068e-01  3.5929e-01  3.5666e-01\n",
            " -1.3602e-01 -2.6616e-01  5.1051e-02 -3.8901e-01 -7.1688e-01  4.2142e-01\n",
            "  3.4138e-01  2.0226e-01  7.2525e-01 -8.2843e-02 -5.0313e-04  9.6399e-02\n",
            " -9.9015e-01 -2.5072e-01  1.3442e-01 -1.4179e-01  4.5943e-01  1.4141e-01\n",
            "  2.2300e-01  2.5430e-01 -1.4116e-01  1.6485e-01  3.6200e-01  2.7605e-01\n",
            "  4.2591e-01 -7.7762e-01 -3.0578e-01 -3.2377e-01 -1.4614e-01 -3.0976e-01\n",
            "  2.0135e-02  8.5477e-01 -4.5221e-01  2.2912e-01 -1.6643e-02 -8.9288e-02\n",
            "  5.5455e-01 -1.7363e-01  2.6326e-01  2.5399e-01 -6.9577e-01  1.2545e+00\n",
            "  3.8360e-01  6.1884e-01 -1.9624e-01 -6.2276e-01  2.8007e-01 -3.0718e-01\n",
            " -4.5880e-01 -6.4558e-02  1.2010e-01 -3.7269e-01  2.3845e-02  9.3502e-01\n",
            "  6.9278e-01  3.9504e-01 -1.6516e-01  2.6946e-01 -2.2277e-01 -4.2757e-01\n",
            "  2.2187e-01 -1.9731e-01 -4.3778e-01  4.5320e-01 -1.2172e+00  1.9800e-01\n",
            " -1.1947e-01  9.0041e-02  4.2085e-01 -2.0580e-01 -4.3014e-01 -2.3424e-01\n",
            " -2.7362e-01 -6.1372e-01  5.4815e-02 -3.2185e-01  1.4167e-01  1.5766e+00\n",
            " -1.3145e-01  1.4814e-01  5.5194e-01 -2.1592e-01 -4.7255e-01 -2.8905e-01\n",
            "  3.8258e-01 -2.8776e-02 -3.3742e-01 -6.6289e-01  5.9525e-01 -1.1732e-01\n",
            " -6.3757e-02  2.0932e-01  2.7506e-01  9.1853e-03 -2.6480e-02 -9.5169e-01\n",
            " -7.7099e-01  4.8709e-02  5.3862e-01 -1.8867e-01 -2.1653e-01 -1.1950e+00\n",
            " -3.2206e-01  2.0966e-01  2.7707e-01  3.6516e-01 -1.3364e-02  4.9224e-01\n",
            "  1.3911e-01 -6.7729e-01  4.5646e-01  9.0121e-01  1.1623e+00 -2.2724e-02\n",
            "  6.7986e-01  1.8226e-01 -2.3835e-01  2.5362e-01  1.0231e-02  1.1684e-01\n",
            "  3.7366e-01 -6.1280e-01 -1.5657e-01  4.9045e-02  3.4805e-01  1.0667e-01\n",
            "  5.0405e-01 -1.1111e-02  4.3512e-01 -3.5101e-01  6.4006e-02  3.1593e-01\n",
            " -6.2489e-01  2.5810e-01  4.5131e-01 -3.4906e-01  4.8476e-01 -5.0652e-02\n",
            "  6.6812e-01 -2.0489e-01  2.4400e-02  5.1620e-01  7.0946e-02 -7.3994e-03\n",
            " -1.6747e-01  6.8076e-01  3.4721e-01 -2.4126e-02  2.3668e-01 -1.8466e-02\n",
            " -7.9105e-01  1.8090e-01  4.6211e-02 -2.9249e-02 -7.7179e-01 -4.4827e-01\n",
            "  3.0047e-01 -3.5123e-01 -1.9011e-02 -1.1894e-01 -3.5369e-01 -1.2861e-01\n",
            "  1.6508e-01  2.0927e-01  5.5980e-01  3.6686e-01 -1.1306e+00 -1.6466e-01\n",
            "  4.5644e-01 -2.7837e-01]\n"
          ]
        }
      ],
      "source": [
        "word = \"ronald\"\n",
        "if word in embeddings_index:\n",
        "    print(embeddings_index[word])\n",
        "else:\n",
        "    print(\"Word not found in GloVe embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "with open(\"/content/drive/MyDrive/COMS574-Introduction_To_Machine_Learning/News_Category_Dataset_v3.json\", \"r\",encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "#parse json in the files to extract text\n",
        "pattern = '\\{[^{}]*\\}'\n",
        "\n",
        "matches = re.findall(pattern, text)"
      ],
      "metadata": {
        "id": "z_U1gistrpz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#size of dataset\n",
        "len(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjrMNJfUrs53",
        "outputId": "a4532e9f-5a2a-4b39-ea08-7e79d5fcc935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209527"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter\n",
        "!pip install inflect\n",
        "import inflect\n",
        "from multiprocessing import Pool, cpu_count\n",
        "#Also import all nltk libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import words\n",
        "#download and import dictionary of words\n",
        "nltk.download('words')\n",
        "word_list = words.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdW7_HwKrvRM",
        "outputId": "459dfac4-bfb8-4f61-f417-8c5e30cea008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swifter\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.21)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16505 sha256=f43fa8b5eb524c1438966fef9c0cd9f9904aac4a891e26cd5165353c57cb9609\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/7f/bd/9bed48f078f3ee1fa75e0b29b6e0335ce1cb03a38d3443b3a3\n",
            "Successfully built swifter\n",
            "Installing collected packages: swifter\n",
            "Successfully installed swifter-1.4.0\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (7.5.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect) (4.4.2)\n",
            "Requirement already satisfied: typing_extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from typeguard>=4.0.1->inflect) (4.13.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create pandas dataframe out of json file\n",
        "import pandas as pd\n",
        "json_objs=[]\n",
        "for item in matches:\n",
        "  try:\n",
        "     json_obj = json.loads(item)\n",
        "     json_objs.append(json_obj)\n",
        "  except json.JSONDecodeError:\n",
        "     print(f\"Could not decode: {item}\")\n",
        "\n",
        "df = pd.DataFrame(json_objs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoMfIIMxrybP",
        "outputId": "39ae9662-e540-49f8-b690-7d51fca7b993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not decode: {Today's Buddha Doodle}\n",
            "Could not decode: {Today's Buddha Doodle}\n",
            "Could not decode: {This was originally posted to The Screaming\", \"date\": \"2013-08-08\"}\n",
            "Could not decode: {Newlywed}\n",
            "Could not decode: {sic}\n",
            "Could not decode: {EM}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The number of rows in dataframe\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDxVaGDIr07J",
        "outputId": "45f51317-3079-4383-d1ca-e83c524ffca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "209521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#category counts of ecah category\n",
        "value_counts_column = df['category'].value_counts()\n",
        "print(value_counts_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALx0fdZfr3Df",
        "outputId": "91d02509-9fca-4227-dea7-f07331671bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category\n",
            "POLITICS          35602\n",
            "WELLNESS          17945\n",
            "ENTERTAINMENT     17362\n",
            "TRAVEL             9899\n",
            "STYLE & BEAUTY     9813\n",
            "PARENTING          8791\n",
            "HEALTHY LIVING     6694\n",
            "QUEER VOICES       6347\n",
            "FOOD & DRINK       6340\n",
            "BUSINESS           5992\n",
            "COMEDY             5400\n",
            "SPORTS             5077\n",
            "BLACK VOICES       4583\n",
            "HOME & LIVING      4320\n",
            "PARENTS            3955\n",
            "THE WORLDPOST      3664\n",
            "WEDDINGS           3652\n",
            "WOMEN              3572\n",
            "CRIME              3562\n",
            "IMPACT             3484\n",
            "DIVORCE            3426\n",
            "WORLD NEWS         3299\n",
            "MEDIA              2944\n",
            "WEIRD NEWS         2777\n",
            "GREEN              2622\n",
            "WORLDPOST          2579\n",
            "RELIGION           2577\n",
            "STYLE              2254\n",
            "SCIENCE            2206\n",
            "TECH               2104\n",
            "TASTE              2096\n",
            "MONEY              1756\n",
            "ARTS               1509\n",
            "ENVIRONMENT        1444\n",
            "FIFTY              1401\n",
            "GOOD NEWS          1396\n",
            "U.S. NEWS          1377\n",
            "ARTS & CULTURE     1339\n",
            "COLLEGE            1144\n",
            "LATINO VOICES      1130\n",
            "CULTURE & ARTS     1073\n",
            "EDUCATION          1014\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Just taking a subset of 15 categories which are relevant and have good amount of data\n",
        "#Max 5000 rows are selected for eah category.\n",
        "#This is to avoid bias towards particular category.\n",
        "df_subset = pd.DataFrame()\n",
        "categories_shortlisted =['POLITICS','WELLNESS','TRAVEL','STYLE & BEAUTY','SCIENCE','WOMEN','CRIME','BUSINESS','RELIGION'\\\n",
        ",'TECH','U.S. NEWS','WORLD NEWS','ENTERTAINMENT','HEALTHY LIVING','FOOD & DRINK']\n",
        "\n",
        "for category in categories_shortlisted:\n",
        "  temp_df = df[df['category'] == category]\n",
        "  if len(temp_df) >=5000:\n",
        "    temp_df = temp_df.sample(frac=5000/len(temp_df));\n",
        "  df_subset = pd.concat([df_subset,temp_df],ignore_index=True)\n",
        "\n",
        "print(len(df_subset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B_FBkmNr5aD",
        "outputId": "314dfab1-2451-4174-96a1-b0454df78fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "setwords = set(word_list)\n",
        "p = inflect.engine()\n",
        "#If its a dictionary word then just return the lowercase so its not identified as named entity.\n",
        "#This is required for named entity recognition as the dictionary words should not be identified as named entities\n",
        "def checkifItsDictionaryWord(token):\n",
        "   word = copy.deepcopy(token)\n",
        "   #Inorder to check if its a dictionary word, do some preprocessing\n",
        "   #removal of 's or s' for any special characters.\n",
        "   #also remove the inflections such as plural form.\n",
        "   if \"'s\" in word:\n",
        "     word.replace(\"'s\",\"\")\n",
        "   elif \"s'\" in word:\n",
        "     word.replace(\"s'\",\"\")\n",
        "\n",
        "   res = p.singular_noun(word)\n",
        "   #print(res)\n",
        "   if res!=False:\n",
        "      word = res\n",
        "\n",
        "   word = re.sub(r'[^a-zA-Z0-9]', '', word)\n",
        "   if word.lower() in setwords:\n",
        "     del word;\n",
        "     return token.lower()\n",
        "   else:\n",
        "     del word;\n",
        "     return token\n",
        "\n"
      ],
      "metadata": {
        "id": "52U6zKE4r8l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "t-rdzKc9sJwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert words to lower case if its a dictionary word so that its not falsely identified as named entity.\n",
        "def convertWordsToLowerCase(sentence):\n",
        "   tokens = [checkifItsDictionaryWord(token)  for token in sentence.split()]\n",
        "\n",
        "   sentence = \" \".join([token.lower() if token.lower() in setwords else token for token in tokens])\n",
        "   return sentence\n",
        "\n",
        "sentence = \"Cars are best vehicles for Kate's travel\"\n",
        "print(convertWordsToLowerCase(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvup5cXBsNIJ",
        "outputId": "b0560f3c-4f4a-4c87-c50e-3434a81988b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cars are best vehicles for Kate's travel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since all the words in headline are starting with CAPS we want to change it to lower case if its a dictionary word\n",
        "#So that its not falsely identified as named entity\n",
        "df_subset['headline'] = df_subset['headline'].apply(convertWordsToLowerCase)"
      ],
      "metadata": {
        "id": "a_ogWg3ZsT6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally concatenate the headline and short description.\n",
        "df_subset['news_text'] = df_subset['headline']+ \" \"+df_subset['short_description']"
      ],
      "metadata": {
        "id": "kZYQCQLusXd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replaceChars(word):\n",
        "  if \"'s\" in word:\n",
        "     word.replace(\"'s\",\"\")\n",
        "  elif \"s'\" in word:\n",
        "     word.replace(\"s'\",\"\")\n",
        "  return word;\n",
        "\n",
        "def removeApostrophe(sent):\n",
        "   tokens = sent.split()\n",
        "   return \" \".join([replaceChars(token) for token in tokens])\n",
        "\n",
        "df_subset['news_text'] = df_subset['news_text'].apply(removeApostrophe)"
      ],
      "metadata": {
        "id": "X6cvZx15r6Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "#function to remove unicode characters\n",
        "def remove_unicode(text):\n",
        "    #print(text)\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "#function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    #text = \"This is a simple sentence that contains some stop words.\"\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Get the list of stop words in English\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove stop words from the text\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the filtered words back into a sentence\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text\n",
        "\n",
        "#function to perform stemming and lemmatization.\n",
        "def performLemmatizationAndStemming(text):\n",
        "    words = text.split()\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            lemmatized_words.append(lemmatizer.lemmatize(word,pos='v'))\n",
        "        except:\n",
        "            print('exception thrown')\n",
        "            print(word)\n",
        "            sys.exit()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # Assuming verbs for simplicity\n",
        "\n",
        "    # Stemming\n",
        "    #stemmer = PorterStemmer()\n",
        "\n",
        "    #stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
        "    #stemmed_words = ' '.join(stemmed_words)\n",
        "    lemmatized_words = ' '.join(lemmatized_words)\n",
        "    return lemmatized_words\n"
      ],
      "metadata": {
        "id": "jR9K86TEsdm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pYbkDmwiJI3",
        "outputId": "7a22130a-fa3f-43a7-8cc9-f224f2dfcec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#not starting with A-Z or 0-9 delete it\n",
        "df_subset['news_text'] = df_subset['news_text'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
        "#remove unicode\n",
        "df_subset['news_text'] = df_subset['news_text'].apply(remove_unicode)\n",
        "#df_subset['news_text'] = df_subset['news_text'].apply(remove_nondictionary_words)\n",
        "#replace \\n with ''\n",
        "df_subset['news_text'] = df_subset['news_text'].str.replace(r'[\\n]','',regex=True)"
      ],
      "metadata": {
        "id": "r5poWV_3s40O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stop words\n",
        "df_subset['news_text'] = df_subset['news_text'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "_0tSD_6gs7yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text'] = df_subset['news_text'].apply(performLemmatizationAndStemming)"
      ],
      "metadata": {
        "id": "O4wsDbnftENj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_subset['news_text'] = df_subset['news_text'].str.lower()"
      ],
      "metadata": {
        "id": "PZ2NPfdHx0tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset['news_text'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "HB7bXA6Fve4C",
        "outputId": "eaf334b8-0beb-48d1-897b-61ff1902405d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Illinois attorney general ask judge permission...\n",
              "1    quit work trump shes run congress fight Democr...\n",
              "2    years deficit Doomsaying republicans go massiv...\n",
              "3    Texas 12 state push block feds Enforcing Trans...\n",
              "4    Texan city find violation vote right act city ...\n",
              "Name: news_text, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Illinois attorney general ask judge permission...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quit work trump shes run congress fight Democr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>years deficit Doomsaying republicans go massiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Texas 12 state push block feds Enforcing Trans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Texan city find violation vote right act city ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import re\n"
      ],
      "metadata": {
        "id": "e2O1GoTdj59-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = df_subset['news_text'].tolist()"
      ],
      "metadata": {
        "id": "w0Zl0DeykUai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df_subset['category'].tolist()\n",
        "one_hot = pd.get_dummies(df_subset['category'],dtype =float)\n",
        "#rint(one_hot)\n",
        "#sys.exit()\n",
        "one_hot_tensor = one_hot.to_numpy()"
      ],
      "metadata": {
        "id": "fk34gTafRqdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "k3rKPT1AEjeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        print(\"forwarding\")\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "L0HzmBHuE432"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputids = batch['input_ids'].to(device)\n",
        "        attentionmask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        print(\"In model train\")\n",
        "        outputs = model(input_ids=inputids, attention_mask=attentionmask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "jZ8oHUq0Fi7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            inputids = batch['input_ids'].to(device)\n",
        "            attentionmask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=inputids, attention_mask=attentionmask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
      ],
      "metadata": {
        "id": "fFRsn5zlFuw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "RP0Y8xxtKkPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    sentence_list,one_hot_tensor, train_size =0.7,test_size=0.3, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "80oXVn_GnrXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 15"
      ],
      "metadata": {
        "id": "u9ZbDcKlHxeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BERTClassifier(\"google-bert/bert-base-uncased\", num_classes).to(device);\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "sa1MnVZoHiJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "train_dataset = TextClassificationDataset(X_train, y_train, tokenizer, 200)\n",
        "test_dataset = TextClassificationDataset(X_test, y_test, tokenizer, 200)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "lChKdQ7uLett"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "for epoch in range(30):\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "      train(model, train_dataloader, optimizer, device)\n",
        "      accuracy, report = evaluate(model, test_dataloader, device)\n",
        "      print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "      print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BjZmVe7SXaI",
        "outputId": "472ef44c-1c50-4365-a2ee-c00da9f16d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "In model train\n",
            "forwarding\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}